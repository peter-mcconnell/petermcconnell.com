<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Peter McConnell</title>
    <link>https://www.petermcconnell.com/posts/</link>
    <description>Recent content in Posts on Peter McConnell</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 26 Dec 2022 22:54:29 +0000</lastBuildDate><atom:link href="https://www.petermcconnell.com/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Perf engingeering with Python 3.12</title>
      <link>https://www.petermcconnell.com/posts/perf_eng_with_py12/</link>
      <pubDate>Mon, 26 Dec 2022 22:54:29 +0000</pubDate>
      
      <guid>https://www.petermcconnell.com/posts/perf_eng_with_py12/</guid>
      <description>3.12 brings perf profiling! Take a second to go check out https://docs.python.org/3.12/howto/perf_profiling.html and indeed the changelog at https://www.python.org/downloads/release/python-3120a3/
The important part (for this post) from the links above is:
&amp;quot;&amp;quot;&amp;quot; The Linux perf profiler is a very powerful tool that allows you to profile and obtain information about the performance of your application. perf also has a very vibrant ecosystem of tools that aid with the analysis of the data that it produces.</description>
      <content>&lt;p&gt;3.12 brings perf profiling! Take a second to go check out &lt;a href=&#34;https://docs.python.org/3.12/howto/perf_profiling.html&#34;&gt;https://docs.python.org/3.12/howto/perf_profiling.html&lt;/a&gt; and indeed the changelog at &lt;a href=&#34;https://www.python.org/downloads/release/python-3120a3/&#34;&gt;https://www.python.org/downloads/release/python-3120a3/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The important part (for this post) from the links above is:&lt;/p&gt;
&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;
The Linux perf profiler is a very powerful tool that allows you to profile and obtain information about the performance of your application. perf also has a very vibrant ecosystem of tools that aid with the analysis of the data that it produces.&lt;/p&gt;
&lt;p&gt;The main problem with using the perf profiler with Python applications is that perf only allows to get information about native symbols, this is, the names of the functions and procedures written in C. This means that the names and file names of the Python functions in your code will not appear in the output of the perf.&lt;/p&gt;
&lt;p&gt;Since Python 3.12, the interpreter can run in a special mode that allows Python functions to appear in the output of the perf profiler. When this mode is enabled, the interpreter will interpose a small piece of code compiled on the fly before the execution of every Python function and it will teach perf the relationship between this piece of code and the associated Python function using perf map files.
&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;
&lt;h2 id=&#34;writing-a-bad-program&#34;&gt;writing a &amp;ldquo;bad&amp;rdquo; program&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m excited to try this, so lets get going. Firstly lets create a python script for us to profile. I&amp;rsquo;m doing this before installing Python 3.12 as I want to create a FlameGraph of how this process looks in 3.10 verses 3.12. Here we have a script that attempts to perform lookups against a large list:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; time
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;run_dummy&lt;/span&gt;(numbers):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; findme &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;100000&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; findme &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; numbers:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;found&amp;#34;&lt;/span&gt;, findme)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;missed&amp;#34;&lt;/span&gt;, findme)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; __name__ &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;__main__&amp;#34;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# create a large sized input to show off inefficiency&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [i &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;20000000&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    start_time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; time&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;time()  &lt;span style=&#34;color:#75715e&#34;&gt;# get the current time [start]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    run_dummy(numbers)  &lt;span style=&#34;color:#75715e&#34;&gt;# run our inefficient method&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    end_time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; time&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;time()  &lt;span style=&#34;color:#75715e&#34;&gt;# get the current time [end]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    duration &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; end_time &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; start_time  &lt;span style=&#34;color:#75715e&#34;&gt;# Calculate the duration&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Duration: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;duration&lt;span style=&#34;color:#e6db74&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; seconds&amp;#34;&lt;/span&gt;)  &lt;span style=&#34;color:#75715e&#34;&gt;# Print the duration&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Running this I get the following result:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python3.10 assets/dummy/perf_py_proj/before.py
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;found &lt;span style=&#34;color:#ae81ff&#34;&gt;99992&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;found &lt;span style=&#34;color:#ae81ff&#34;&gt;99993&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;found &lt;span style=&#34;color:#ae81ff&#34;&gt;99994&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;found &lt;span style=&#34;color:#ae81ff&#34;&gt;99995&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;found &lt;span style=&#34;color:#ae81ff&#34;&gt;99996&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;found &lt;span style=&#34;color:#ae81ff&#34;&gt;99997&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;found &lt;span style=&#34;color:#ae81ff&#34;&gt;99998&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;found &lt;span style=&#34;color:#ae81ff&#34;&gt;99999&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Duration: 36.06884431838989 seconds
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;36 seconds is bad enough for us to pick up a reasonable amount of samples.&lt;/p&gt;
&lt;h2 id=&#34;flamegraphs&#34;&gt;flamegraphs!&lt;/h2&gt;
&lt;p&gt;Now we can create our &lt;a href=&#34;https://github.com/brendangregg/FlameGraph&#34;&gt;FlameGraph&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# record profile to &amp;#34;perf.data&amp;#34; file (default output)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;perf record -F &lt;span style=&#34;color:#ae81ff&#34;&gt;99&lt;/span&gt; -g -- python3.10 assets/dummy/perf_py_proj/before.py
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# read perf.data (created above) and display trace output&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;perf script &amp;gt; out.perf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# fold stack samples into single lines&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# here I reference ~/FlameGraph/ - you can get this from https://github.com/brendangregg/FlameGraph&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;~/FlameGraph/stackcollapse-perf.pl out.perf &amp;gt; out.folded
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# generate flamegraph&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;~/FlameGraph/flamegraph.pl out.folded &amp;gt; ./assets/perf_example_python3.10.svg
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This gives us a nice SVG that visualises the traces:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/petermcconnell.com/master/assets/perf_example_python3.10.svg&#34; alt=&#34;python 3.10 perf flamegraph&#34; title=&#34;python 3.10 perf flamegraph&#34;&gt;&lt;/p&gt;
&lt;p&gt;This isn&amp;rsquo;t useful &amp;hellip; I can see most of the time was spent in &amp;ldquo;new_keys_object.lto_priv.0&amp;rdquo; but that is meaningless in the context of the code.&lt;/p&gt;
&lt;h2 id=&#34;time-for-python-312&#34;&gt;Time for Python 3.12&amp;hellip;&lt;/h2&gt;
&lt;p&gt;First I need to install it - the steps for this vary depending on OS - follow the build instructions here for your environment: &lt;a href=&#34;https://github.com/python/cpython/tree/v3.12.0a3#build-instructions&#34;&gt;https://github.com/python/cpython/tree/v3.12.0a3#build-instructions&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# for me on ubuntu:22.04&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;export CFLAGS&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;-fno-omit-frame-pointer -mno-omit-leaf-frame-pointer&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;./configure --enable-optimizations
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;make
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;make test
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo make install
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;unset CFLAGS
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# after this I reset my systems python3 symlink to 3.10 as 3.12 isn&amp;#39;t yet stable&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# for testing python3.12 I&amp;#39;ll call &amp;#34;python3.12&amp;#34; instead of &amp;#34;python3&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ln -sf /usr/local/bin/python3.10 /usr/local/bin/python3
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With that installed I first need to enable perf support. This is detailed in &lt;a href=&#34;https://docs.python.org/3.12/howto/perf_profiling.html&#34;&gt;https://docs.python.org/3.12/howto/perf_profiling.html&lt;/a&gt; and there are three options: 1) an environment variable, 2) an -X option or 3) dynamically using &lt;code&gt;sys&lt;/code&gt;. I&amp;rsquo;ll go for the environment variable approach as I don&amp;rsquo;t mind &lt;em&gt;everything&lt;/em&gt; being profiled for a small script:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;export PYTHONPERFSUPPORT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now we simply repeat the process above using the &lt;code&gt;python3.12&lt;/code&gt; binary instead:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# record profile to &amp;#34;perf.data&amp;#34; file (default output)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;perf record -F &lt;span style=&#34;color:#ae81ff&#34;&gt;99&lt;/span&gt; -g -- python3.12 assets/dummy/perf_py_proj/before.py
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# read perf.data (created above) and display trace output&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;perf script &amp;gt; out.perf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# fold stack samples into single lines&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# here I reference ~/FlameGraph/ - you can get this from https://github.com/brendangregg/FlameGraph&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;~/FlameGraph/stackcollapse-perf.pl out.perf &amp;gt; out.folded
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# generate flamegraph&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;~/FlameGraph/flamegraph.pl out.folded &amp;gt; ./assets/perf_example_python3.12.before.svg
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;First we&amp;rsquo;ll take a peek at the report with &lt;code&gt;perf report -g -i perf.data&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/petermcconnell.com/master/assets/perf_report.png&#34; alt=&#34;python 3.12 perf report output&#34; title=&#34;python 3.12 perf report&#34;&gt;&lt;/p&gt;
&lt;p&gt;Awesome! We can see our Python function names and script names!&lt;/p&gt;
&lt;p&gt;Now we can take a look at the updated SVG that visualises the traces with Python 3.12:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/petermcconnell.com/master/assets/perf_example_python3.12.before.svg&#34; alt=&#34;python 3.12 perf flamegraph&#34; title=&#34;python 3.12 perf flamegraph&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is already looking much more useful. We see the majority of the time is being spent doing comparisons and in the list_contains method. We can also see the specific file &lt;code&gt;before.py&lt;/code&gt; and method &lt;code&gt;run_dummy&lt;/code&gt; that is calling it.&lt;/p&gt;
&lt;h2 id=&#34;investigation-time--the-fix&#34;&gt;Investigation time / the fix&lt;/h2&gt;
&lt;p&gt;Now that we know where in our code the problem is, we can take a look at the source code in CPython to see why the &lt;code&gt;list_contains&lt;/code&gt; method would be so slow: &lt;a href=&#34;https://github.com/python/cpython/blob/199507b81a302ea19f93593965b1e5088195a6c5/Objects/listobject.c#L440&#34;&gt;https://github.com/python/cpython/blob/199507b81a302ea19f93593965b1e5088195a6c5/Objects/listobject.c#L440&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// I found this by going to https://github.com/python/cpython/ and searching for &amp;#34;list_contains&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;static&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;list_contains&lt;/span&gt;(PyListObject &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;a, PyObject &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;el)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    PyObject &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;item;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    Py_ssize_t i;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; cmp;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, cmp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; ; cmp &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Py_SIZE&lt;/span&gt;(a); &lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;i) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        item &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;PyList_GET_ITEM&lt;/span&gt;(a, i);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a6e22e&#34;&gt;Py_INCREF&lt;/span&gt;(item);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        cmp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;PyObject_RichCompareBool&lt;/span&gt;(item, el, Py_EQ);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#a6e22e&#34;&gt;Py_DECREF&lt;/span&gt;(item);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; cmp;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Nasty&amp;hellip; looking at this code I can see that each time it is invoked it iterates over the array and performs a comparison against each item. That&amp;rsquo;s far from ideal for our usecase, so lets go back to the Python code we wrote. Our Flamegraph shows us that the problem is in our &lt;code&gt;run_dummy&lt;/code&gt; method:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;run_dummy&lt;/span&gt;(numbers):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; findme &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;100000&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; findme &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; numbers:  &lt;span style=&#34;color:#75715e&#34;&gt;#  &amp;lt;- this is what triggers list_contains&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;found&amp;#34;&lt;/span&gt;, findme)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;missed&amp;#34;&lt;/span&gt;, findme)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can&amp;rsquo;t really change that line as it is doing what we want it to do - identifying if an integer is in &lt;code&gt;numbers&lt;/code&gt;. Perhaps we can change the &lt;code&gt;numbers&lt;/code&gt; data type to one better suited to lookups. In our existing code we have:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    numbers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [i &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;20000000&lt;/span&gt;)]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    start_time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; time&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;time()  &lt;span style=&#34;color:#75715e&#34;&gt;# get the current time [start]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    run_dummy(numbers)  &lt;span style=&#34;color:#75715e&#34;&gt;# run our inefficient method&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Here we used a LIST data type for our &amp;ldquo;numbers&amp;rdquo;, which under the hood (in CPython) is implemented as dynamically-sized arrays and as such are nowhere near as efficient (O(N)) as the likes of a Hashtable for looking up an item (which is O(1)). A SET on the other hand (another Python data type) is implemented as a Hashtable and would give us the fast lookup we&amp;rsquo;re looking for. Lets change the data type in our Python code and see what the impact is:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# we&amp;#39;ll just change this line, casting numbers to a set before running run_dummy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    run_dummy(set(numbers))  &lt;span style=&#34;color:#75715e&#34;&gt;# passing a set() for fast lookups&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now we can repeat the steps as above to generate our new flamegraph:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# record profile to &amp;#34;perf.data&amp;#34; file (default output)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;perf record -F &lt;span style=&#34;color:#ae81ff&#34;&gt;99&lt;/span&gt; -g -- python3.12 assets/dummy/perf_py_proj/after.py
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;found &lt;span style=&#34;color:#ae81ff&#34;&gt;99998&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;found &lt;span style=&#34;color:#ae81ff&#34;&gt;99999&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Duration: 0.8350753784179688 seconds
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt; perf record: Woken up &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; times to write data &lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt; perf record: Captured and wrote 0.039 MB perf.data &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;134&lt;/span&gt; samples&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Already we can see that things have massively improved. Where previously this was taking 36 seconds to run it is now taking 0.8 seconds! Lets continue creating our flamegraph for the new improved code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# read perf.data (created above) and display trace output&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;perf script &amp;gt; out.perf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# fold stack samples into single lines&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# here I reference ~/FlameGraph/ - you can get this from https://github.com/brendangregg/FlameGraph&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;~/FlameGraph/stackcollapse-perf.pl out.perf &amp;gt; out.folded
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# generate flamegraph&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;~/FlameGraph/flamegraph.pl out.folded &amp;gt; ./assets/perf_example_python3.12.after.svg
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/petermcconnell.com/master/assets/perf_example_python3.12.after.svg&#34; alt=&#34;python 3.12 perf flamegraph improved&#34; title=&#34;python 3.12 perf flamegraph improved&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is a much healthier looking Flamegraph and our application is now much faster as a result. The perf profiling support in Python 3.12 brings a tremendously useful tool to software engineers that want to deliver fast programs and I&amp;rsquo;m excited to see the impact this will have on the language.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Linux performance ideas</title>
      <link>https://www.petermcconnell.com/posts/binary_perf_summary/</link>
      <pubDate>Mon, 26 Dec 2022 15:36:09 +0000</pubDate>
      
      <guid>https://www.petermcconnell.com/posts/binary_perf_summary/</guid>
      <description>Optimizing the Performance of your Linux Binaries When it comes to software performance, every little bit counts. This is especially true for Linux binaries, which are often used in resource-constrained environments or where every millisecond of performance matters. In this article, we&amp;rsquo;ll explore some tips for optimizing the performance of your Linux binaries.
Tip 1: Use a high-performance compiler The first step to optimizing the performance of your Linux binaries is to use a high-performance compiler.</description>
      <content>&lt;h1 id=&#34;optimizing-the-performance-of-your-linux-binaries&#34;&gt;Optimizing the Performance of your Linux Binaries&lt;/h1&gt;
&lt;p&gt;When it comes to software performance, every little bit counts. This is especially true for Linux binaries, which are often used in resource-constrained environments or where every millisecond of performance matters. In this article, we&amp;rsquo;ll explore some tips for optimizing the performance of your Linux binaries.&lt;/p&gt;
&lt;h2 id=&#34;tip-1-use-a-high-performance-compiler&#34;&gt;Tip 1: Use a high-performance compiler&lt;/h2&gt;
&lt;p&gt;The first step to optimizing the performance of your Linux binaries is to use a high-performance compiler. Modern compilers like GCC and Clang have various optimization flags that can significantly improve the performance of your code. For example, the &amp;ldquo;-O2&amp;rdquo; flag enables a variety of optimization techniques that can improve the performance of your code, while the &amp;ldquo;-Ofast&amp;rdquo; flag enables even more aggressive optimizations that can further improve performance but may result in code that is less standards-compliant.&lt;/p&gt;
&lt;h2 id=&#34;tip-2-use-a-faster-runtime&#34;&gt;Tip 2: Use a faster runtime&lt;/h2&gt;
&lt;p&gt;The runtime environment of your Linux binary can also have a significant impact on its performance. For example, using a faster interpreter or just-in-time (JIT) compiler can improve the performance of interpreted or JIT-compiled languages like Python, JavaScript, and Java. You can also consider using a faster garbage collector to reduce the overhead of memory management.&lt;/p&gt;
&lt;h2 id=&#34;tip-3-optimize-your-algorithms-and-data-structures&#34;&gt;Tip 3: Optimize your algorithms and data structures&lt;/h2&gt;
&lt;p&gt;The algorithms and data structures you use can also have a big impact on the performance of your Linux binaries. Choosing the right algorithm and data structure for the task at hand can significantly improve the performance of your code. For example, using a hash table instead of a linked list for lookups can greatly improve the performance of your code.&lt;/p&gt;
&lt;h2 id=&#34;tip-4-use-profiling-tools&#34;&gt;Tip 4: Use profiling tools&lt;/h2&gt;
&lt;p&gt;Profiling tools can help you identify areas of your code that are causing performance bottlenecks. For example, you can use tools like perf, Valgrind, and gprof to measure the performance of your code and identify areas that could be optimized.&lt;/p&gt;
&lt;h2 id=&#34;tip-5-use-a-performance-tuned-linux-distribution&#34;&gt;Tip 5: Use a performance-tuned Linux distribution&lt;/h2&gt;
&lt;p&gt;Finally, using a performance-tuned Linux distribution can also improve the performance of your Linux binaries. Distributions like Arch Linux and Gentoo are known for their focus on performance and can provide a lightweight and optimized environment for your applications to run in.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Optimizing the performance of your Linux binaries requires a combination of tools and techniques. By using a high-performance compiler, a faster runtime, optimizing your algorithms and data structures, using profiling tools, and using a performance-tuned Linux distribution, you can significantly improve the performance of your Linux binaries and deliver better results to your users.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>website perf - a few common performance improvement ideas for web applications</title>
      <link>https://www.petermcconnell.com/posts/website_perf/</link>
      <pubDate>Mon, 26 Dec 2022 15:30:38 +0000</pubDate>
      
      <guid>https://www.petermcconnell.com/posts/website_perf/</guid>
      <description>5 Tips for Improving the Performance of your Web Applications Performance is critical to the success of any application. Poor performance can lead to a poor user experience, reduced engagement, and ultimately, decreased revenue. In this article, we&amp;rsquo;ll explore five tips for improving the performance of your applications.
Tip 1: Measure and monitor performance regularly The first step to improving performance is understanding where the bottlenecks are. To do this, you need to measure and monitor performance regularly.</description>
      <content>&lt;h1 id=&#34;5-tips-for-improving-the-performance-of-your-web-applications&#34;&gt;5 Tips for Improving the Performance of your Web Applications&lt;/h1&gt;
&lt;p&gt;Performance is critical to the success of any application. Poor performance can lead to a poor user experience, reduced engagement, and ultimately, decreased revenue. In this article, we&amp;rsquo;ll explore five tips for improving the performance of your applications.&lt;/p&gt;
&lt;h2 id=&#34;tip-1-measure-and-monitor-performance-regularly&#34;&gt;Tip 1: Measure and monitor performance regularly&lt;/h2&gt;
&lt;p&gt;The first step to improving performance is understanding where the bottlenecks are. To do this, you need to measure and monitor performance regularly. Use tools like profilers, load testers, and monitoring tools to gather data on the performance of your application. This will help you identify areas that are causing slowdowns and prioritize improvements. Setting up a visualisation tool for this, such as Grafana, will mean that you can get this data whenever you need it.&lt;/p&gt;
&lt;h2 id=&#34;tip-2-optimize-database-queries&#34;&gt;Tip 2: Optimize database queries&lt;/h2&gt;
&lt;p&gt;Databases are often a major source of performance issues. To optimize database queries, consider using indexing to speed up query execution, use caching to store frequently accessed data, and use explain plans to understand how queries are being executed. You should also consider using a database optimization tool to identify and fix common issues. You should ensure your query caching stragegy is sound for your application.&lt;/p&gt;
&lt;h2 id=&#34;tip-3-use-a-cdn-for-static-content&#34;&gt;Tip 3: Use a CDN for static content&lt;/h2&gt;
&lt;p&gt;Content Delivery Networks (CDNs) are a great way to improve the performance of your application. CDNs store static content like images, CSS, and JavaScript on servers around the world, so that users can access them from a location that is closer to them. This can significantly reduce the time it takes for static content to load, resulting in a faster overall page load time.&lt;/p&gt;
&lt;h2 id=&#34;tip-4-optimize-front-end-performance&#34;&gt;Tip 4: Optimize front-end performance&lt;/h2&gt;
&lt;p&gt;The front-end of your application plays a critical role in performance. To optimize front-end performance, consider using techniques like minification, compression, and lazy loading. You should also consider using a tool like Webpack to bundle and optimize your assets.&lt;/p&gt;
&lt;h2 id=&#34;tip-5-use-a-load-balancer&#34;&gt;Tip 5: Use a load balancer&lt;/h2&gt;
&lt;p&gt;If you have a high traffic application, using a load balancer can help distribute traffic across multiple servers, which can improve the performance of your application. Load balancers can also help improve the reliability of your application by automatically routing traffic away from overloaded servers.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Improving the performance of your application requires a combination of tools and techniques. By regularly measuring and monitoring performance, optimizing database queries, using a CDN for static content, optimizing front-end performance, and using a load balancer, you can significantly improve the performance of your application and provide a better user experience.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Monolith vs Microservice repositories - some pros and cons</title>
      <link>https://www.petermcconnell.com/posts/monolith_vs_microservice_repos/</link>
      <pubDate>Mon, 26 Dec 2022 15:01:29 +0000</pubDate>
      
      <guid>https://www.petermcconnell.com/posts/monolith_vs_microservice_repos/</guid>
      <description>Monolithic vs Microservice repository management Here we&amp;rsquo;ll take a quick look at monolithic vs microservice repository architecture for managing codebases.
A monolithic repository is a version control system that stores all of the code for a software application in a single, centralized repository. This means that all of the code for an application, including the front-end, back-end, and any additional components, is stored in a single location and managed as a cohesive unit.</description>
      <content>&lt;h1 id=&#34;monolithic-vs-microservice-repository-management&#34;&gt;Monolithic vs Microservice repository management&lt;/h1&gt;
&lt;p&gt;Here we&amp;rsquo;ll take a quick look at monolithic vs microservice repository architecture for managing codebases.&lt;/p&gt;
&lt;p&gt;A monolithic repository is a version control system that stores all of the code for a software application in a single, centralized repository. This means that all of the code for an application, including the front-end, back-end, and any additional components, is stored in a single location and managed as a cohesive unit.&lt;/p&gt;
&lt;p&gt;On the other hand, a microservice repository is a version control system that stores the code for a software application in multiple, smaller repositories, with each repository containing a specific service or component of the application. This means that the code for an application is divided into smaller, more focused units, each with its own repository.&lt;/p&gt;
&lt;p&gt;There are several pros and cons to consider when deciding between a monolithic repository and a microservice repository for your software development project.&lt;/p&gt;
&lt;h2 id=&#34;pros-of-monolithic-repositories&#34;&gt;Pros of Monolithic Repositories&lt;/h2&gt;
&lt;p&gt;Simplicity: Monolithic repositories are simpler to set up and manage than microservice repositories. With all of the code for an application stored in a single location, it is easier to navigate and work with. It also helps implement &amp;ldquo;standard&amp;rdquo; tools, org-wide. Generating reports, such as license compliance, is also a lot more simple.&lt;/p&gt;
&lt;p&gt;Fewer Dependencies: In a monolithic repository, all of the code for an application is stored in a single location, which means that there are fewer dependencies between different components of the application. This can make it easier to understand how the different parts of the application fit together and how changes to one part might affect other parts of the application.&lt;/p&gt;
&lt;p&gt;Easier to Test: Testing a monolithic repository is generally easier than testing a microservice repository because all of the code is stored in a single location. This means that it is easier to set up test environments and run tests on the entire application.&lt;/p&gt;
&lt;h2 id=&#34;cons-of-monolithic-repositories&#34;&gt;Cons of Monolithic Repositories&lt;/h2&gt;
&lt;p&gt;Complexity: As an application grows and becomes more complex, a monolithic repository can become difficult to manage and maintain. With all of the code stored in a single location, it can be challenging to understand how different parts of the application fit together and how changes to one part might affect other parts of the application. Furthermore, trying to map this complexity to appropriate owners can be difficult to setup and maintain.&lt;/p&gt;
&lt;p&gt;Slow Deployment: Because a monolithic repository contains all of the code for an application, deploying updates or changes to the application can be slow and cumbersome. This can be particularly problematic for large applications with many dependencies and integrations. Care needs to be taken in the build tooling to avoid unnecessary cycles.&lt;/p&gt;
&lt;p&gt;Politics / Co-ownership: service owners are no longer repository administrators by default. Implementing change to the general build tooling, for example, requires careful coordination amongst external teams.&lt;/p&gt;
&lt;h2 id=&#34;pros-of-microservice-repositories&#34;&gt;Pros of Microservice Repositories&lt;/h2&gt;
&lt;p&gt;Strong Ownership: with microservice repositories teams can manage their own repos, implementing the tooling which best fits the need for their team without having to consult too many external teams beforehand.&lt;/p&gt;
&lt;p&gt;Improved Deployment: Because each service or component of an application is stored in a separate repository, it is easier to deploy updates or changes to a specific service or component without affecting the rest of the application. It allows the CI/CD configuration to be very lean. This can make deployment faster and more efficient. It&amp;rsquo;s also more common here for service teams to own their CI/CD pipelines.&lt;/p&gt;
&lt;p&gt;Better Organization: With each service or component of an application stored in a separate repository, it is easier to understand how different parts of the application fit together and how changes to one part might affect other parts of the application. This can improve organization and make it easier to manage and maintain the application. At the least it requires less cognitive load to understand parts of the system on their own.&lt;/p&gt;
&lt;h2 id=&#34;cons-of-microservice-repositories&#34;&gt;Cons of Microservice Repositories&lt;/h2&gt;
&lt;p&gt;Complexity: Microservice repositories can be more complex to set up and manage than monolithic repositories. With each service or component of an application stored in a separate repository, there are more dependencies and integrations to manage and maintain.&lt;/p&gt;
&lt;p&gt;More Dependencies: With each service or component of an application stored in their own repositories it&amp;rsquo;s harder to control the external dependencies the entire system is using. For example, repo A may use version 1.0.1 of a library, whilst repo B uses 1.0.2. Standardizing on libraries and binaries is very difficult and the result can be a lot of bloat to the sytem as a whole.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Having seen both in the wild my recommendation would be to go with a monolithic repository if you have people to throw at the tooling - this will make some eyes roll and implementing change to parts of the repo which affect all teams will feel slow, but frankly this is a good pain to feel - without it, people will choose the path of least resistence which often results in growing tech debt, system bloat and disparity in standards and pulling that back together retrospectively can be painful.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>ChatGPT for software engineers</title>
      <link>https://www.petermcconnell.com/posts/chatgpt_for_engineers/</link>
      <pubDate>Thu, 08 Dec 2022 14:07:06 +0000</pubDate>
      
      <guid>https://www.petermcconnell.com/posts/chatgpt_for_engineers/</guid>
      <description>ChatGPT, the open-source version of the GPT-3 language model developed by OpenAI, has the potential to revolutionize the way senior software engineers approach their daily workflow. With its ability to generate human-like text, ChatGPT can help senior software engineers save time and reduce errors in a variety of tasks, from coding and debugging to documenting and collaborating with colleagues.
One of the key advantages of using ChatGPT is its ability to generate high-quality code.</description>
      <content>&lt;p&gt;ChatGPT, the open-source version of the GPT-3 language model developed by OpenAI, has the potential to revolutionize the way senior software engineers approach their daily workflow. With its ability to generate human-like text, ChatGPT can help senior software engineers save time and reduce errors in a variety of tasks, from coding and debugging to documenting and collaborating with colleagues.&lt;/p&gt;
&lt;p&gt;One of the key advantages of using ChatGPT is its ability to generate high-quality code. Senior software engineers can use ChatGPT to quickly prototype new features, test out different approaches to solving a problem, or even generate entire codebases for new projects. This can save senior software engineers a significant amount of time and allow them to focus on more complex and challenging tasks.&lt;/p&gt;
&lt;p&gt;In addition to generating code, ChatGPT can also help senior software engineers with debugging and troubleshooting. By providing natural language descriptions of bugs and errors, senior software engineers can use ChatGPT to generate possible solutions or workarounds. This can save them the time and frustration of trying to figure out a solution on their own, and allow them to quickly get back to work.&lt;/p&gt;
&lt;p&gt;Another important way that ChatGPT can help senior software engineers is by improving the documentation of their code and projects. With its ability to generate human-like text, ChatGPT can help senior software engineers write clear and concise documentation that is easy for others to understand. This can improve collaboration within teams and ensure that everyone is on the same page when working on a project.&lt;/p&gt;
&lt;p&gt;In conclusion, ChatGPT has the potential to greatly improve the daily workflow of senior software engineers. By helping them generate high-quality code, troubleshoot problems, and write clear documentation, ChatGPT can save senior software engineers time and reduce errors, allowing them to focus on more important tasks.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;this article was generated entirely by ChatGPT&lt;/em&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>chatGPT - building an automated database testing tool</title>
      <link>https://www.petermcconnell.com/posts/ai_db_testing/</link>
      <pubDate>Thu, 08 Dec 2022 11:41:50 +0000</pubDate>
      
      <guid>https://www.petermcconnell.com/posts/ai_db_testing/</guid>
      <description>Creating an automated database testing tool with ChatGPT Last night I thought I&amp;rsquo;d try to get ChatGPT to make an automated database testing tool and the results were quite promising.
In conclusion, with guidance, it was able to build a project from scratch that ran a python script and postgres database. It generated some random schema and values for the randomly generated tables. It provided a Python script which would introspect the database and execute queries against it.</description>
      <content>&lt;h2 id=&#34;creating-an-automated-database-testing-tool-with-chatgpt&#34;&gt;Creating an automated database testing tool with ChatGPT&lt;/h2&gt;
&lt;p&gt;Last night I thought I&amp;rsquo;d try to get ChatGPT to make an automated database
testing tool and the results were quite promising.&lt;/p&gt;
&lt;p&gt;In conclusion, with guidance, it was able to build a project from scratch that
ran a python script and postgres database. It generated some random schema and
values for the randomly generated tables. It provided a Python script which
would introspect the database and execute queries against it.&lt;/p&gt;
&lt;p&gt;Did it all work out of the box? No. There are some bugs to fix in the python
script it generated. However the effort to go in and fix those is not high and
certainly the whole end-to-end process is cheaper, time-wise, compared to
starting from scratch.&lt;/p&gt;
&lt;p&gt;I found that the bugs it encountered were largely due to my lack of clarity or
ordering of questions posed to it. It was quite capable of fixing its own
mistakes / updating the existing code to match the new requirements when
requested to do so.&lt;/p&gt;
&lt;p&gt;The only &lt;em&gt;real&lt;/em&gt; issue I encountered were general API errors that one would
expect of something so popular in an early preview state.&lt;/p&gt;
&lt;p&gt;I came away from this experiment viewing ChatGPT and whatever follows it as a
really useful development aide for those who already know how to program. It
helped me build a tool faster than I could have had I sat down to do it from
scratch. I don&amp;rsquo;t view it as a replacement for software engineers yet for two
main reasons - firstly: for non-trivial applications I suspect the person
feeding requirements into the system (or &amp;ldquo;prompt engineer&amp;rdquo;) needs to have a
reasonable idea of how to build software in the first place, so as to know how
to form requests and to correct mistakes / close gaps. secondly: the code being
generated isn&amp;rsquo;t always sound - without an experienced engineer reviewing and
taking ownership of whatever code is produced (ownership being important for
maintainence reasons) then there&amp;rsquo;s little guarantee that you will get what you
are hoping for.&lt;/p&gt;
&lt;p&gt;However; this is still very early days. Can the problems outlined be closed
further? Absolutely. Will this sort of tooling be &amp;ldquo;bad&amp;rdquo; for software
engineering as a whole, long-term? Perhaps. Personally I&amp;rsquo;m very excited to have
this tool in my arsenal - already it has allowed me to scaffold prototype
applications quickly. Would I use it for production code in a workplace? No
more or less than I would snippets from stackoverflow or it&amp;rsquo;s ilk. For now.&lt;/p&gt;
&lt;p&gt;Github repository: &lt;a href=&#34;https://github.com/peter-mcconnell/gpt_sql_test_generator&#34;&gt;https://github.com/peter-mcconnell/gpt_sql_test_generator&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Screenshots:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/2.png&#34; alt=&#34;step 2&#34; title=&#34;step 2&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/3.png&#34; alt=&#34;step 3&#34; title=&#34;step 3&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/4.png&#34; alt=&#34;step 4&#34; title=&#34;step 4&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/5.png&#34; alt=&#34;step 5&#34; title=&#34;step 5&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/6.png&#34; alt=&#34;step 6&#34; title=&#34;step 6&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/7.png&#34; alt=&#34;step 7&#34; title=&#34;step 7&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/8.png&#34; alt=&#34;step 8&#34; title=&#34;step 8&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/9.png&#34; alt=&#34;step 9&#34; title=&#34;step 9&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/10.png&#34; alt=&#34;step 10&#34; title=&#34;step 10&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/11.png&#34; alt=&#34;step 11&#34; title=&#34;step 11&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/12.png&#34; alt=&#34;step 12&#34; title=&#34;step 12&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/13.png&#34; alt=&#34;step 13&#34; title=&#34;step 13&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/14.png&#34; alt=&#34;step 14&#34; title=&#34;step 14&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/15.png&#34; alt=&#34;step 15&#34; title=&#34;step 15&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/peter-mcconnell/gpt_sql_test_generator/master/media/16.png&#34; alt=&#34;step 16&#34; title=&#34;step 16&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>chatGPT - building a virtual machine</title>
      <link>https://www.petermcconnell.com/posts/chatgpt/</link>
      <pubDate>Mon, 05 Dec 2022 14:16:04 +0000</pubDate>
      
      <guid>https://www.petermcconnell.com/posts/chatgpt/</guid>
      <description>:mind-blown:
https://www.engraved.blog/building-a-virtual-machine-inside/</description>
      <content>&lt;p&gt;:mind-blown:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.engraved.blog/building-a-virtual-machine-inside/&#34;&gt;https://www.engraved.blog/building-a-virtual-machine-inside/&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Vimtips</title>
      <link>https://www.petermcconnell.com/posts/vimtips/</link>
      <pubDate>Thu, 01 Dec 2022 11:36:49 +0000</pubDate>
      
      <guid>https://www.petermcconnell.com/posts/vimtips/</guid>
      <description>I stumbled across a nice vim tips resource today:
https://thevaluable.dev/vim-advanced/</description>
      <content>&lt;p&gt;I stumbled across a nice vim tips resource today:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://thevaluable.dev/vim-advanced/&#34;&gt;https://thevaluable.dev/vim-advanced/&lt;/a&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Hugo</title>
      <link>https://www.petermcconnell.com/posts/hugo/</link>
      <pubDate>Tue, 29 Nov 2022 23:16:21 +0000</pubDate>
      
      <guid>https://www.petermcconnell.com/posts/hugo/</guid>
      <description>How I set this website up, for free tldr; Hugo + Github + AWS Amplify. Easy peasy. total time: 1 hour (mostly spent writing content)
Some parameters:
free templates. I&amp;rsquo;m no designer static web asset output is fine. I&amp;rsquo;m not building a backend (yet) but having the option to do so is a bonus this project should be as low effort as possible so; no html / css / js where possible also; automated deploys on push to $branch ideally I don&amp;rsquo;t need to maintain ci to do this.</description>
      <content>&lt;h2 id=&#34;how-i-set-this-website-up-for-free&#34;&gt;How I set this website up, for free&lt;/h2&gt;
&lt;p&gt;tldr; Hugo + Github + AWS Amplify. Easy peasy.
total time: 1 hour (mostly spent writing content)&lt;/p&gt;
&lt;p&gt;Some parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;free templates. I&amp;rsquo;m no designer&lt;/li&gt;
&lt;li&gt;static web asset output is fine. I&amp;rsquo;m not building a backend (yet)
&lt;ul&gt;
&lt;li&gt;but having the option to do so is a bonus&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;this project should be as low effort as possible
&lt;ul&gt;
&lt;li&gt;so; no html / css / js where possible&lt;/li&gt;
&lt;li&gt;also; automated deploys on push to $branch
&lt;ul&gt;
&lt;li&gt;ideally I don&amp;rsquo;t need to maintain ci to do this. lowest poss. effort&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;as cheap to host as possible. free, ideally&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary I want to only write page content. No code. No pipelines. No buttons
to click. But I still want uri&amp;rsquo;s, the abililty to render rich media and a pretty
template that I need to do nothing with. And I&amp;rsquo;d like to not pay for any of it.&lt;/p&gt;
&lt;h3 id=&#34;static-web-files&#34;&gt;static web files&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;ve heard about Hugo for years but never had the opportunity to try it out and
knew from the criteria I had that it should solve getting me the static assets
quickly. Even if it didn&amp;rsquo;t turn out to be the &lt;em&gt;right&lt;/em&gt; tool, I knew
experimentation would be cheap.&lt;/p&gt;
&lt;p&gt;Install was super easy: &lt;a href=&#34;https://gohugo.io/installation/linux/&#34;&gt;https://gohugo.io/installation/linux/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I then jumped over to &lt;a href=&#34;https://gohugo.io/getting-started/quick-start/&#34;&gt;https://gohugo.io/getting-started/quick-start/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This quick start guide felt like all I&amp;rsquo;d need so I went searching for a theme.
The power of search engines brought me to &lt;a href=&#34;https://hugothemesfree.com/&#34;&gt;https://hugothemesfree.com/&lt;/a&gt;. The tags
on the right of this page helped me find the terminal theme quickly
(&lt;a href=&#34;https://hugothemesfree.com/a-simple-retro-theme-for-hugo/&#34;&gt;https://hugothemesfree.com/a-simple-retro-theme-for-hugo/&lt;/a&gt;) which reminded me
of my old i3 + polybar configuration. MIT licensed too. Bingo.&lt;/p&gt;
&lt;p&gt;A few &lt;code&gt;hugo new posts/thing.md&lt;/code&gt; and &lt;code&gt;hugo new otherthing.md&lt;/code&gt;&amp;rsquo;s later and I had
my static website files. I opted to bake the theme into the repo so that I
could mutate the files. Created a new repository
(&lt;a href=&#34;https://github.com/peter-mcconnell/petermcconnell.com&#34;&gt;https://github.com/peter-mcconnell/petermcconnell.com&lt;/a&gt;) and threw my files
there for safe keeping. Now I just needed somewhere to host it.&lt;/p&gt;
&lt;h3 id=&#34;a-search-for-cheap-hosting-solutions&#34;&gt;a search for cheap hosting solutions&lt;/h3&gt;
&lt;p&gt;The most obvious route was github pages but I wanted to look at other options
which offered some extra features should I need them in the future.&lt;/p&gt;
&lt;p&gt;A quick look around lead me to AWS Amplify - a service I admittedly hadn&amp;rsquo;t
heard of before. A quick look over the marketing material
(&lt;a href=&#34;https://aws.amazon.com/getting-started/hands-on/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-1/&#34;&gt;https://aws.amazon.com/getting-started/hands-on/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-1/&lt;/a&gt;)
looked like it was interesting; Lambda, API Gateway, Dynamo DB - all things
that pluck on my cheap-skate heart strings. Pricing:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/amplify/pricing/&#34;&gt;https://aws.amazon.com/amplify/pricing/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m no Madonna - I think my personal website is probably a safe bet to do
&amp;ldquo;free tier&amp;rdquo; numbers (&amp;lt;500k req.|15Gb egress|100Gb req. duration per month).&lt;/p&gt;
&lt;p&gt;I logged into my personal AWS account, went through the little Amplify setup
wizard, pointed it to my github repo, updated my DNS records to whatever the
wizard was telling me to and voila - &lt;a href=&#34;https://www.petermcconnell.com/&#34;&gt;https://www.petermcconnell.com/&lt;/a&gt; is up and
running.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Current Reading</title>
      <link>https://www.petermcconnell.com/posts/current-reading/</link>
      <pubDate>Tue, 29 Nov 2022 20:30:52 +0000</pubDate>
      
      <guid>https://www.petermcconnell.com/posts/current-reading/</guid>
      <description>The Linux Programming Interface I should have read this years ago. It&amp;rsquo;s a big boi, but if you work around linux or are interested in it - this book is a critical primer on many of the core topics you&amp;rsquo;ll need to really understand what&amp;rsquo;s going on. I binged this book on the first pass over multiple weeks - deliberately skipping implementing the code examples, opting to just read them / ensure I understood them.</description>
      <content>&lt;h2 id=&#34;the-linux-programming-interface&#34;&gt;The Linux Programming Interface&lt;/h2&gt;
&lt;p&gt;I should have read this years ago. It&amp;rsquo;s a big boi, but if you work around linux
or are interested in it - this book is a critical primer on many of the core
topics you&amp;rsquo;ll need to really understand what&amp;rsquo;s going on. I binged this book on
the first pass over multiple weeks - deliberately skipping implementing the
code examples, opting to just read them / ensure I understood them. I choose to
do this so that I could get over as much content as possible, with the view of
circling back over the examples once I&amp;rsquo;m finished The C Programming Language
book so that I can make better decisions about the code I&amp;rsquo;m writing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s.cdnshm.com/catalog/pt/t/33820519/linux-programming-interface.jpg&#34; alt=&#34;alt text&#34; title=&#34;Buy it&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.co.uk/Linux-Programming-Interface-System-Handbook/dp/1593272200&#34;&gt;https://www.amazon.co.uk/Linux-Programming-Interface-System-Handbook/dp/1593272200&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;the-c-programming-language&#34;&gt;The C Programming Language&lt;/h2&gt;
&lt;p&gt;I generally try to avoid taking on multiple books but I figured as the above is
so large and much of the books example code is given in C, then this book which
I&amp;rsquo;m long overdue on would be a nice complement.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s fine. Concise and clear wording; unlike the print quality, unfortunately.
This books print looks like it was hand-painted by a very nervous individual trying
to hit a deadline in the dark. Print-quality aside, I&amp;rsquo;d say this is an
important book to read, but not much of a page-turner. For me the biggest
advantage of the book was seeing syscall libraries in action and also
appreciating more about what cpp gives us.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://m.media-amazon.com/images/I/C1bOAdsnZnS._CR504,0,3024,3024_UX256.jpg&#34; alt=&#34;alt text&#34; title=&#34;Buy it&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.co.uk/C-Programming-Language-2nd/dp/0131103628&#34;&gt;https://www.amazon.co.uk/C-Programming-Language-2nd/dp/0131103628&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;up-next&#34;&gt;Up next&lt;/h2&gt;
&lt;p&gt;I have two &amp;ldquo;crackers&amp;rdquo; on my desk. Really looking forward to these:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linux Kernel Development (&lt;a href=&#34;https://www.amazon.co.uk/Linux-Kernel-Development-Developers-Library/dp/0672329468&#34;&gt;https://www.amazon.co.uk/Linux-Kernel-Development-Developers-Library/dp/0672329468&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Systems Performance (&lt;a href=&#34;https://www.amazon.co.uk/Systems-Performance-Brendan-Gregg/dp/0136820158&#34;&gt;https://www.amazon.co.uk/Systems-Performance-Brendan-Gregg/dp/0136820158&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
  </channel>
</rss>
